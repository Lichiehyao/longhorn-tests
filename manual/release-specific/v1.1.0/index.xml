<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>v1.1.0 on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/</link>
    <description>Recent content in v1.1.0 on Longhorn Manual Test Cases</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Longhorn upgrade with node down and removal</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/longhorn-upgrade-with-node-down-and-removal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/longhorn-upgrade-with-node-down-and-removal/</guid>
      <description>Longhorn upgrade with node down and removal  Launch Longhorn v1.0.x Create and attach a volume, then write data to the volume. Directly remove a Kubernetes node, and shut down a node. Wait for the related replicas failure. Then record replica.Spec.DiskID for the failed replicas. Upgrade to Longhorn master Verify the Longhorn node related to the removed node is gone. Verify  replica.Spec.DiskID on the down node is updated and the field of the replica on the gone node is unchanged.</description>
    </item>
    
    <item>
      <title>Prometheus Support</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/prometheus_support/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/prometheus_support/</guid>
      <description>Prometheus Support allows user to monitor the longhorn metrics. The details are available at https://longhorn.io/docs/1.1.0/monitoring/
Monitor longhorn  Deploy the Prometheus-operator, ServiceMonitor pointing to longhorn-backend and Prometheus as mentioned in the doc. Create an ingress pointing to Prometheus service. Access the Prometheus web UI using the ingress created in the step 2. Select the metrics from below to monitor the longhorn resources.  longhorn_volume_capacity_bytes longhorn_volume_actual_size_bytes longhorn_volume_state longhorn_volume_robustness longhorn_instance_manager_cpu_requests_millicpu longhorn_instance_manager_cpu_usage_millicpu longhorn_instance_manager_memory_requests_bytes longhorn_instance_manager_memory_usage_bytes longhorn_manager_cpu_usage_millicpu longhorn_manager_memory_usage_bytes longhorn_node_count_total longhorn_node_status longhorn_node_cpu_capacity_millicpu longhorn_node_cpu_usage_millicpu longhorn_node_memory_capacity_bytes longhorn_node_memory_usage_bytes   Deploy workloads which use Longhorn volumes into the cluster.</description>
    </item>
    
    <item>
      <title>Support Kubelet Volume Metrics</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/kubelet_volume_metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/kubelet_volume_metrics/</guid>
      <description>Intro Kubelet exposes kubelet_volume_stats_* metrics. Those metrics measure PVC&amp;rsquo;s filesystem related information inside a Longhorn block device.
Test steps:  Create a cluster and set up this monitoring system: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack Install Longhorn. Deploy some workloads using Longhorn volumes. Make sure there are some workloads using Longhorn PVCs in volumeMode: Block and some workloads using Longhorn PVCs in volumeMode: Filesystem. See https://longhorn.io/docs/1.0.2/references/examples/ for examples. Create ingress to Prometheus server and Grafana. Navigate to Prometheus server, verify that all Longhorn PVCs in volumeMode: Filesystem show up in metrics: kubelet_volume_stats_capacity_bytes kubelet_volume_stats_available_bytes kubelet_volume_stats_used_bytes kubelet_volume_stats_inodes kubelet_volume_stats_inodes_free kubelet_volume_stats_inodes_used.</description>
    </item>
    
    <item>
      <title>Test Additional Printer Columns</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/additional-printer-columns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/additional-printer-columns/</guid>
      <description>For each of the case below:
 Fresh installation of Longhorn. (make sure to delete all Longhorn CRDs before installation) Upgrade from older version.  Run:
kubectl get &amp;lt;LONGHORN-CRD&amp;gt; -n longhorn-system Verify that the output contains information as specify in the additionalPrinerColumns at here</description>
    </item>
    
    <item>
      <title>Test Instance Manager IP Sync</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/instance-manager-ip-sync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/instance-manager-ip-sync/</guid>
      <description>Test step:  Launch longhorn system Create and attach a volume Follow this doc to manually modify the IP of one instance-manager-r. e.g., curl -k -XPATCH -H &amp;quot;Accept: application/json&amp;quot; -H &amp;quot;Content-Type: application/merge-patch+json&amp;quot; -H &amp;quot;Authorization: Bearer kubeconfig-xxxxxx&amp;quot; --data &#39;{&amp;quot;status&amp;quot;:{&amp;quot;ip&amp;quot;:&amp;quot;1.1.1.1&amp;quot;}}&#39; https://172.104.72.64/k8s/clusters/c-znrxc/apis/longhorn.io/v1beta1/namespaces/longhorn-system/instancemanagers/instance-manager-r-63ece607/status  Notice that the bearer token kubeconfig-xxx can be found in your kube config file Remember to add /status at the end of the URL   Verify the IP of the instance manager still matches the pod IP Verify the volume can be detached.</description>
    </item>
    
    <item>
      <title>Test ISCSI Installation on EKS</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/iscsi_installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/iscsi_installation/</guid>
      <description>This is for EKS or similar users who doesn&amp;rsquo;t need to log into each host to install &amp;lsquo;ISCSI&amp;rsquo; individually.
Test steps:
 Create an EKS cluster with 3 nodes. Run the following command to install iscsi on every nodes.  kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/iscsi/longhorn-iscsi-installation.yaml In Longhorn Manager Repo Directory run:  kubectl apply -Rf ./deploy/install/ Longhorn should be able installed successfully. Try to create a pod with a pvc:  kubectl apply -f https://raw.</description>
    </item>
    
    <item>
      <title>Test uninstallation</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/uninstallation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/uninstallation/</guid>
      <description>Launch Longhorn system.
  Use scripts to continuously create then delete multiple DaemonSets.
 e.g., putting the following python test into the manager integration test directory and run it:  from common import get_apps_api_client # NOQA def test_uninstall_script(): apps_api = get_apps_api_client() while True: for i in range(10): name = &amp;quot;ds-&amp;quot; + str(i) try: ds = apps_api.read_namespaced_daemon_set(name, &amp;quot;default&amp;quot;) if ds.status.number_ready == ds.status.number_ready: apps_api.delete_namespaced_daemon_set(name, &amp;quot;default&amp;quot;) except Exception: apps_api.create_namespaced_daemon_set( &amp;quot;default&amp;quot;, ds_manifest(name)) def ds_manifest(name): return { &#39;apiVersion&#39;: &#39;apps/v1&#39;, &#39;kind&#39;: &#39;DaemonSet&#39;, &#39;metadata&#39;: { &#39;name&#39;: name }, &#39;spec&#39;: { &#39;selector&#39;: { &#39;matchLabels&#39;: { &#39;app&#39;: name } }, &#39;template&#39;: { &#39;metadata&#39;: { &#39;labels&#39;: { &#39;app&#39;: name } }, &#39;spec&#39;: { &#39;terminationGracePeriodSeconds&#39;: 10, &#39;containers&#39;: [{ &#39;image&#39;: &#39;busybox&#39;, &#39;imagePullPolicy&#39;: &#39;IfNotPresent&#39;, &#39;name&#39;: &#39;sleep&#39;, &#39;args&#39;: [ &#39;/bin/sh&#39;, &#39;-c&#39;, &#39;while true;do date;sleep 5; done&#39; ], }] } }, } }   Start to uninstall longhorn.</description>
    </item>
    
    <item>
      <title>Upgrade Longhorn with modified Storage Class</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/upgrade_with_modified_storageclass/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/upgrade_with_modified_storageclass/</guid>
      <description>Intro Longhorn can be upgraded with modified Storage Class.
Related Issue https://github.com/longhorn/longhorn/issues/1527
Test steps: Kubectl apply -f  Install Longhorn v1.0.2 kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.0.2/deploy/longhorn.yaml  Create a statefulset using longhorn storageclass for PVCs. Set the scale to 1. Observe that there is a workload pod (pod-1) is using 1 volume (vol-1) with 3 replicas. In Longhorn repo, on master branch. Modify numberOfReplicas: &amp;quot;1&amp;quot; in https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml. Upgrade Longhorn to master by running kubectl apply -f https://raw.</description>
    </item>
    
  </channel>
</rss>
