<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>v1.1.1 on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/</link>
    <description>Recent content in v1.1.1 on Longhorn Manual Test Cases</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Test Disable IPv6</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2136 https://github.com/longhorn/longhorn/issues/2197
Longhorn v1.1.1 should work with IPv6 disabled.
Scenario  Install Kubernetes Disable IPv6 on all the worker nodes using the following  Go to the folder /etc/default In the grub file, edit the value GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;ipv6.disable=1&amp;quot; Once the file is saved update by the command update-grub Reboot the node and once the node becomes active, use the command cat /proc/cmdline to verify &amp;quot;ipv6.disable=1&amp;quot; is reflected in the values Deploy Longhorn and test basic use cases.</description>
    </item>
    
    <item>
      <title>Test Snapshot Purge Error Handling</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/1895
Longhorn v1.1.1 handles the error during snapshot purge better and reports to Longhorn-manager.
Scenario-1  Create a volume with 3 replicas and attach to a pod. Write some data into the volume and take a snapshot. Delete a replica that will result in creating a system generated snapshot. Wait for replica to finish and take another snapshot. ssh into a node and resize the latest snapshot. (e.g dd if=/dev/urandom count=50 bs=1M of=&amp;lt;SNAPSHOT-NAME&amp;gt;.</description>
    </item>
    
  </channel>
</rss>
