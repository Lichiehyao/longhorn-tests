<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>v1.1.1 on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/</link>
    <description>Recent content in v1.1.1 on Longhorn Manual Test Cases</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CSI Sanity Check</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/csi-sanity-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/csi-sanity-check/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2076
Run csi-sanity   Prepare Longhorn cluster and setup backup target.
  Make csi-sanity binary from csi-test.
  On one of the cluster node, run csi-sanity binary.
csi-sanity -csi.endpoint /var/lib/kubelet/obsoleted-longhorn-plugins/driver.longhorn.io/csi.sock -ginkgo.skip=&amp;#34;should create volume from an existing source snapshot|should return appropriate values|should succeed when creating snapshot with maximum-length name|should succeed when requesting to create a snapshot with already existing name and same source volume ID|should fail when requesting to create a snapshot with already existing name and different source volume ID&amp;#34;  NOTE</description>
    </item>
    
    <item>
      <title>Longhorn with engine is not deployed on all the nodes</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/partial-engine-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/partial-engine-deployment/</guid>
      <description>Related Issue https://github.com/longhorn/longhorn/issues/2081
Scenarios: Case 1: Test volume operations when some of the engine image DaemonSet pods are miss scheduled  Install Longhorn in a 3-node cluster: node-1, node-2, node-3 Create a volume, vol-1, of 3 replicas Create another volume, vol-2, of 3 replicas Taint node-1 with the taint: key=value:NoSchedule Check that all functions (attach, detach, snapshot, backup, expand, restore, creating DR volume, &amp;hellip; ) are working ok for vol-1  Case 2: Test volume operations when some of engine image DaemonSet pods are not fully deployed  Continue from case 1 Attach vol-1 to node-1.</description>
    </item>
    
    <item>
      <title>Set Tolerations/PriorityClass For System Components</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/tolerations_priorityclass_setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/tolerations_priorityclass_setting/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2120
Manual Tests:
Case 1: Existing Longhorn installation  Install Longhorn master. Change toleration in UI setting Verify that longhorn.io/last-applied-tolerations annotation and toleration of manager, drive deployer, UI are not changed. Verify that longhorn.io/last-applied-tolerations annotation and toleration for managed components (CSI components, IM pods, share manager pod, EI daemonset, backing-image-manager, cronjob) are updated correctly  Case 2: New installation by Helm  Install Longhorn master, set tolerations like:  defaultSettings: taintToleration: &amp;#34;key=value:NoSchedule&amp;#34; longhornManager: priorityClass: ~ tolerations: - key: key operator: Equal value: value effect: NoSchedule longhornDriver: priorityClass: ~ tolerations: - key: key operator: Equal value: value effect: NoSchedule longhornUI: priorityClass: ~ tolerations: - key: key operator: Equal value: value effect: NoSchedule  Verify that the toleration is added for: IM pods, Share Manager pods, CSI deployments, CSI daemonset, the backup jobs, manager, drive deployer, UI Uninstall the Helm release.</description>
    </item>
    
    <item>
      <title>Test Disable IPv6</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2136
https://github.com/longhorn/longhorn/issues/2197
Longhorn v1.1.1 should work with IPv6 disabled.
Scenario  Install Kubernetes Disable IPv6 on all the worker nodes using the following Go to the folder /etc/default In the grub file, edit the value GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;ipv6.disable=1&amp;quot; Once the file is saved update by the command update-grub Reboot the node and once the node becomes active, Use the command cat /proc/cmdline to verify &amp;quot;ipv6.disable=1&amp;quot; is reflected in the values  Deploy Longhorn and test basic use cases.</description>
    </item>
    
    <item>
      <title>Test Frontend Traffic</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/ws-traffic-flood/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/ws-traffic-flood/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2372
Test Frontend Traffic Given 100 pvc created.
And all pvcs deployed and detached.
When monitor traffic in frontend pod with nload.
apk add nload nload Then should not see a continuing large amount of traffic when there is no operation happening. The smaller spikes are mostly coming from event resources which possibly could be enhanced later (https://github.com/longhorn/longhorn/issues/2433).</description>
    </item>
    
    <item>
      <title>Test Node Delete</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/delete-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/delete-node/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2186 https://github.com/longhorn/longhorn/issues/2462
Delete Method Should verify with both of the delete methods.
 Bulk Delete - This is the Delete on the Node page. Node Delete - This is the Remove Node for each node Operation drop-down list.  Test Node Delete - should grey out when node not down Given node not Down.
When Try to delete any node.
Then Should see button greyed out.
Test Node Delete Given pod with pvc created.</description>
    </item>
    
    <item>
      <title>Test RWX share-mount ownership reset</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/rwx-mount-ownership-reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/rwx-mount-ownership-reset/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2357
Test RWX share-mount ownership Given Setup one of cluster node to use host FQDN.
root@ip-172-30-0-139:/home/ubuntu# cat /etc/hosts 127.0.0.1 localhost 54.255.224.72 ip-172-30-0-139.lan ip-172-30-0-139 root@ip-172-30-0-139:/home/ubuntu# hostname ip-172-30-0-139 root@ip-172-30-0-139:/home/ubuntu# hostname -f ip-172-30-0-139.lan And Domain = localdomain is commented out in /etc/idmapd.conf on cluster hosts. This is to ensure localdomain is not enforce to sync between server and client. Ref: https://github.com/longhorn/website/pull/279
root@ip-172-30-0-139:~# cat /etc/idmapd.conf [General] Verbosity = 0 Pipefs-Directory = /run/rpc_pipefs # set your own domain here, if it differs from FQDN minus hostname # Domain = localdomain [Mapping] Nobody-User = nobody Nobody-Group = nogroup And pod with rwx pvc deployed to the node with host FQDN.</description>
    </item>
    
    <item>
      <title>Test Service Account mount on host</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-service-account-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-service-account-mount/</guid>
      <description>This test case should be tested on both yaml installation, chart installation (Helm and Rancher UI), as well as upgrade scenarios After install Longhorn using on of the above method, ssh into a worker node that has a longhorn-manager pod running check the mount point /run/secrets/kubernetes.io/serviceaccount by running: root@node-1:~# findmnt /run/secrets/kubernetes.io/serviceaccount  Verify that there is no such mount point Kill the longhorn-manager pod on the above node and wait for it to be recreated and running check the mount point /run/secrets/kubernetes.</description>
    </item>
    
    <item>
      <title>Test Snapshot Purge Error Handling</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/1895
Longhorn v1.1.1 handles the error during snapshot purge better and reports to Longhorn-manager.
Scenario-1  Create a volume with 3 replicas and attach to a pod. Write some data into the volume and take a snapshot. Delete a replica that will result in creating a system generated snapshot. Wait for replica to finish and take another snapshot. ssh into a node and resize the latest snapshot. (e.g dd if=/dev/urandom count=50 bs=1M of=&amp;lt;SNAPSHOT-NAME&amp;gt;.</description>
    </item>
    
    <item>
      <title>Test system upgrade with the deprecated CPU setting</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2207
Test step  Deploy a cluster that each node has different CPUs. Launch Longhorn v1.1.0. Deploy some workloads using Longhorn volumes. Upgrade to the latest Longhorn version. Validate:  all workloads work fine and no instance manager pod crash during the upgrade. The fields node.Spec.EngineManagerCPURequest and node.Spec.ReplicaManagerCPURequest of each node are the same as the setting Guaranteed Engine CPU value in the old version * 1000. The old setting Guaranteed Engine CPU is deprecated with an empty value.</description>
    </item>
    
    <item>
      <title>Test Volume Backup and Restore by AWS IAM role</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/aws-iam-role-for-volume-backup-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/aws-iam-role-for-volume-backup-restore/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/1526
Longhorn v1.1.1 should work with volume backup and restore by AWS IAM role
Scenario  Install AWS CLI and configure your AWS credentials  curl &amp;#34;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip&amp;#34; -o &amp;#34;awscli-bundle.zip&amp;#34; unzip awscli-bundle.zip ./awscli-bundle/install -b ~/bin/aws ~/bin/aws configure Create AWS S3 bucket  S3_BUCKET_NAME=&amp;lt;bucket-name&amp;gt; ~/bin/aws s3api create-bucket \  --bucket $S3_BUCKET_NAME \  --region us-west-2 \  --create-bucket-configuration LocationConstraint=us-west-2 Create a new IAM instance profile NodeInstanceProfile  cat &amp;gt; node-instance-assume-role-policy.json &amp;lt;&amp;lt;EOF { &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Principal&amp;#34;: { &amp;#34;Service&amp;#34;: &amp;#34;ec2.</description>
    </item>
    
  </channel>
</rss>
