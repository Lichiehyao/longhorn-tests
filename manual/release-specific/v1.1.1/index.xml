<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>v1.1.1 on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/</link>
    <description>Recent content in v1.1.1 on Longhorn Manual Test Cases</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CSI Sanity Check</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/csi-sanity-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/csi-sanity-check/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2076
Run csi-sanity   Prepare Longhorn cluster and setup backup target.
  Make csi-sanity binary from csi-test.
  On one of the cluster node, run csi-sanity binary.
csi-sanity -csi.endpoint /var/lib/kubelet/obsoleted-longhorn-plugins/driver.longhorn.io/csi.sock -ginkgo.skip=&amp;#34;should create volume from an existing source snapshot|should return appropriate values|should succeed when creating snapshot with maximum-length name|should succeed when requesting to create a snapshot with already existing name and same source volume ID|should fail when requesting to create a snapshot with already existing name and different source volume ID&amp;#34;  NOTE</description>
    </item>
    
    <item>
      <title>Test Disable IPv6</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2136 https://github.com/longhorn/longhorn/issues/2197
Longhorn v1.1.1 should work with IPv6 disabled.
Scenario  Install Kubernetes Disable IPv6 on all the worker nodes using the following  Go to the folder /etc/default In the grub file, edit the value GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;ipv6.disable=1&amp;quot; Once the file is saved update by the command update-grub Reboot the node and once the node becomes active, use the command cat /proc/cmdline to verify &amp;quot;ipv6.disable=1&amp;quot; is reflected in the values Deploy Longhorn and test basic use cases.</description>
    </item>
    
    <item>
      <title>Test Frontend Traffic</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/ws-traffic-flood/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/ws-traffic-flood/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2372
Test Frontend Traffic Given 100 pvc created.
And all pvcs deployed and detached.
When monitor traffic in frontend pod with nload.
apk add nload nload Then should not see a continuing large amount of traffic when there is no operation happening. The smaller spikes are mostly coming from event resources which possibly could be enhanced later (https://github.com/longhorn/longhorn/issues/2433).</description>
    </item>
    
    <item>
      <title>Test Node Delete</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/delete-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/delete-node/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2186
Test Node Delete Given pod with pvc created.
kubectl create -f https://raw.githubusercontent.com/longhorn/longhorn/v1.1.0/examples/pod_with_pvc.yaml And node down:
  Disable Node Scheduling and set Eviction Requested to true from the browser.
  Taint node-1 with kubectl and wait for pods to re-deploy.
kubectl taint nodes ${NODE} nodetype=storage:NoExecute 2.1. Check longhorn pods are not scheduled to node-1.
2.2. Node status should be Down.
  When node-1 deleted from the browser.</description>
    </item>
    
    <item>
      <title>Test Snapshot Purge Error Handling</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/1895
Longhorn v1.1.1 handles the error during snapshot purge better and reports to Longhorn-manager.
Scenario-1  Create a volume with 3 replicas and attach to a pod. Write some data into the volume and take a snapshot. Delete a replica that will result in creating a system generated snapshot. Wait for replica to finish and take another snapshot. ssh into a node and resize the latest snapshot. (e.g dd if=/dev/urandom count=50 bs=1M of=&amp;lt;SNAPSHOT-NAME&amp;gt;.</description>
    </item>
    
    <item>
      <title>Test system upgrade with the deprecated CPU setting</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/2207
Test step  Deploy a cluster that each node has different CPUs. Launch Longhorn v1.1.0. Deploy some workloads using Longhorn volumes. Upgrade to the latest Longhorn version. Validate:  all workloads work fine and no instance manager pod crash during the upgrade. The fields node.Spec.EngineManagerCPURequest and node.Spec.ReplicaManagerCPURequest of each node are the same as the setting Guaranteed Engine CPU value in the old version * 1000. The old setting Guaranteed Engine CPU is deprecated with an empty value.</description>
    </item>
    
    <item>
      <title>Test Volume Backup and Restore by AWS IAM role</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/aws-iam-role-for-volume-backup-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/aws-iam-role-for-volume-backup-restore/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/1526
Longhorn v1.1.1 should work with volume backup and restore by AWS IAM role
Scenario  Install AWS CLI and configure your AWS credentials  curl &amp;#34;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip&amp;#34; -o &amp;#34;awscli-bundle.zip&amp;#34; unzip awscli-bundle.zip ./awscli-bundle/install -b ~/bin/aws ~/bin/aws configure Create AWS S3 bucket  S3_BUCKET_NAME=&amp;lt;bucket-name&amp;gt; ~/bin/aws s3api create-bucket \  --bucket $S3_BUCKET_NAME \  --region us-west-2 \  --create-bucket-configuration LocationConstraint=us-west-2 Create a new IAM instance profile NodeInstanceProfile  cat &amp;gt; node-instance-assume-role-policy.json &amp;lt;&amp;lt;EOF { &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Principal&amp;#34;: { &amp;#34;Service&amp;#34;: &amp;#34;ec2.</description>
    </item>
    
  </channel>
</rss>
