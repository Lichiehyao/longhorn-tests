<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>tests.test_recurring_job API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_recurring_job</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pytest
import time
import json

from datetime import datetime

import common
from common import client, clients, core_api, apps_api  # NOQA
from common import random_labels, volume_name  # NOQA
from common import storage_class, statefulset  # NOQA
from common import make_deployment_with_pvc  # NOQA
from common import cleanup_volume, wait_for_volume_delete
from common import create_storage_class, \
    create_and_wait_statefulset, delete_and_wait_pv
from common import update_statefulset_manifests, get_self_host_id, \
    get_statefulset_pod_info, wait_volume_kubernetes_status
from common import set_random_backupstore
from common import write_volume_random_data
from common import write_pod_volume_random_data
from common import BASE_IMAGE_LABEL, KUBERNETES_STATUS_LABEL
from common import SIZE, Mi, Gi
from common import SETTING_RECURRING_JOB_WHILE_VOLUME_DETACHED


RECURRING_JOB_LABEL = &#34;RecurringJob&#34;
RECURRING_JOB_NAME = &#34;backup&#34;
MAX_BACKUP_STATUS_SIZE = 5


def create_jobs1():
    # snapshot every one minute
    job_snap = {&#34;name&#34;: &#34;snap&#34;, &#34;cron&#34;: &#34;* * * * *&#34;,
                &#34;task&#34;: &#34;snapshot&#34;, &#34;retain&#34;: 2}
    # backup every two minutes
    job_backup = {&#34;name&#34;: &#34;backup&#34;, &#34;cron&#34;: &#34;*/2 * * * *&#34;,
                  &#34;task&#34;: &#34;backup&#34;, &#34;retain&#34;: 1}
    return [job_snap, job_backup]


def check_jobs1_result(volume):
    snapshots = volume.snapshotList()
    count = 0
    for snapshot in snapshots:
        if snapshot.removed is False:
            count += 1
    # 2 snapshots, 1 backup, 1 volume-head
    assert count == 4


def wait_until_begin_of_an_even_minute():
    while True:
        current_time = datetime.utcnow()
        if current_time.second == 0 and current_time.minute % 2 == 0:
            break
        time.sleep(1)

@pytest.mark.recurring_job  # NOQA
def test_recurring_job(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test recurring job

    1. Setup a random backupstore
    2. Create a volume.
    3. Create two jobs
        1 job 1: snapshot every one minute, retain 2
        1 job 2: backup every two minutes, retain 1
    4. Attach the volume.
       Wait until the 10th second since the beginning of an even minute
    5. Write some data. Sleep 2.5 minutes.
       Write some data. Sleep 2.5 minutes
    6. Verify we have 4 snapshots total
        1. 2 snapshots, 1 backup, 1 volume-head
    7. Update jobs to replace the backup job
        1. New backup job run every one minute, retain 2
    8. Write some data. Sleep 2.5 minutes.
       Write some data. Sleep 2.5 minutes
    9. We should have 6 snapshots
        1. 2 from job_snap, 1 from job_backup, 2 from job_backup2, 1
        volume-head
    10. Make sure there are exactly 4 completed backups.
        1. old backup job completed 2 backups
        2. new backup job completed 2 backups
    11. Make sure we have no backup in progress
    &#34;&#34;&#34;

    &#39;&#39;&#39;
    The timeline looks like this:
    0   1   2   3   4   5   6   7   8   9   10     (minute)
    |W  |   | W |   |   |W  |   | W |   |   |      (write data)
    |   S   |   S   |   |   S   |   S   |   |      (job_snap)
    |   |   B   |   B   |   |   |   |   |   |      (job_backup1)
    |   |   |   |   |   |   B   |   B   |   |      (job_backup2)
    &#39;&#39;&#39;

    host_id = get_self_host_id()

    set_random_backupstore(client)

    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=2)
    volume = common.wait_for_volume_detached(client, volume_name)

    jobs = create_jobs1()
    volume.recurringUpdate(jobs=jobs)

    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    # wait until the beginning of an even minute
    wait_until_begin_of_an_even_minute()
    # wait until the 10th second of an even minute
    # make sure that snapshot job happens before the backup job
    time.sleep(10)

    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes
    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes

    check_jobs1_result(volume)

    job_backup2 = {&#34;name&#34;: &#34;backup2&#34;, &#34;cron&#34;: &#34;* * * * *&#34;,
                   &#34;task&#34;: &#34;backup&#34;, &#34;retain&#34;: 2}
    volume.recurringUpdate(jobs=[jobs[0], job_backup2])

    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes
    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes

    snapshots = volume.snapshotList()
    count = 0
    for snapshot in snapshots:
        if snapshot.removed is False:
            count += 1
    # 2 from job_snap, 1 from job_backup, 2 from job_backup2, 1 volume-head
    assert count == 6

    complete_backup_number = 0
    in_progress_backup_number = 0
    volume = client.by_id_volume(volume_name)
    for b in volume.backupStatus:
        assert b.error == &#34;&#34;
        if b.state == &#34;complete&#34;:
            complete_backup_number += 1
        elif b.state == &#34;in_progress&#34;:
            in_progress_backup_number += 1

    # 2 completed backups from job_backup
    # 2 completed backups from job_backup2
    assert complete_backup_number == 4

    assert in_progress_backup_number == 0

    volume = volume.detach()

    common.wait_for_volume_detached(client, volume_name)

    client.delete(volume)

    wait_for_volume_delete(client, volume_name)

    volumes = client.list_volume()
    assert len(volumes) == 0


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_in_volume_creation(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test create volume with recurring jobs

    1. Create volume with recurring jobs though Longhorn API
    2. Verify the recurring jobs run correctly
    &#34;&#34;&#34;
    host_id = get_self_host_id()

    set_random_backupstore(client)

    # error when creating volume with duplicate jobs
    with pytest.raises(Exception) as e:
        client.create_volume(name=volume_name, size=SIZE,
                             numberOfReplicas=2,
                             recurringJobs=create_jobs1() + create_jobs1())
    assert &#34;duplicate job&#34; in str(e.value)

    client.create_volume(name=volume_name, size=SIZE,
                         numberOfReplicas=2, recurringJobs=create_jobs1())
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    # wait until the beginning of an even minute
    wait_until_begin_of_an_even_minute()
    # wait until the 10th second of an even minute
    # to avoid writing data at the same time backup is taking
    time.sleep(10)

    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes
    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes

    check_jobs1_result(volume)

    volume = volume.detach()
    common.wait_for_volume_detached(client, volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    volumes = client.list_volume()
    assert len(volumes) == 0


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_in_storageclass(client, core_api, storage_class, statefulset):  # NOQA
    &#34;&#34;&#34;
    Test create volume with StorageClass contains recurring jobs

    1. Create a StorageClass with recurring jobs
    2. Create a StatefulSet with PVC template and StorageClass
    3. Verify the recurring jobs run correctly.
    &#34;&#34;&#34;
    set_random_backupstore(client)
    statefulset_name = &#39;recurring-job-in-storageclass-test&#39;
    update_statefulset_manifests(statefulset, storage_class, statefulset_name)
    storage_class[&#34;parameters&#34;][&#34;recurringJobs&#34;] = json.dumps(create_jobs1())

    create_storage_class(storage_class)

    # wait until the beginning of an even minute
    wait_until_begin_of_an_even_minute()

    start_time = datetime.utcnow()
    create_and_wait_statefulset(statefulset)
    statefulset_creating_duration = datetime.utcnow() - start_time

    assert 150 &gt; statefulset_creating_duration.seconds

    # We want to write data exactly at the 150th second since the start_time
    time.sleep(150 - statefulset_creating_duration.seconds)

    pod_info = get_statefulset_pod_info(core_api, statefulset)
    volume_info = [p[&#39;pv_name&#39;] for p in pod_info]
    pod_names = [p[&#39;pod_name&#39;] for p in pod_info]

    # write random data to volume to trigger recurring snapshot and backup job
    volume_data_path = &#34;/data/test&#34;
    for pod_name in pod_names:
        write_pod_volume_random_data(core_api, pod_name, volume_data_path, 2)

    time.sleep(150)  # 2.5 minutes

    for volume_name in volume_info:  # NOQA
        volume = client.by_id_volume(volume_name)
        check_jobs1_result(volume)


@pytest.mark.recurring_job
def test_recurring_job_labels(client, random_labels, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test a RecurringJob with labels

    1. Set a random backupstore
    2. Create a backup recurring job with labels
    3. Wait for job to create a backup
    4. Add a label to the job
    5. Verify the recurring jobs run correctly.
    6. Verify the labels on the backup are correct.
    &#34;&#34;&#34;
    recurring_job_labels_test(client, random_labels, volume_name)


def recurring_job_labels_test(client, labels, volume_name, size=SIZE, base_image=&#34;&#34;):  # NOQA
    set_random_backupstore(client)
    host_id = get_self_host_id()
    client.create_volume(name=volume_name, size=size,
                         numberOfReplicas=2)
    volume = common.wait_for_volume_detached(client, volume_name)

    # Simple Backup Job that runs every 1 minute, retains 1.
    jobs = [
        {
            &#34;name&#34;: RECURRING_JOB_NAME,
            &#34;cron&#34;: &#34;*/1 * * * *&#34;,
            &#34;task&#34;: &#34;backup&#34;,
            &#34;retain&#34;: 1,
            &#34;labels&#34;: labels
        }
    ]
    volume.recurringUpdate(jobs=jobs)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)
    write_volume_random_data(volume)

    # 1 minutes 15s
    time.sleep(75)
    labels[&#34;we-added-this-label&#34;] = &#34;definitely&#34;
    jobs[0][&#34;labels&#34;] = labels
    volume = volume.recurringUpdate(jobs=jobs)
    volume = common.wait_for_volume_healthy(client, volume_name)
    write_volume_random_data(volume)

    # 2 minutes 15s
    time.sleep(135)
    snapshots = volume.snapshotList()
    count = 0
    for snapshot in snapshots:
        if snapshot.removed is False:
            count += 1
    # 1 from Backup, 1 from Volume Head.
    assert count == 2

    # Verify the Labels on the actual Backup.
    bv = client.by_id_backupVolume(volume_name)
    backups = bv.backupList().data
    assert len(backups) == 1

    b = bv.backupGet(name=backups[0].name)
    for key, val in iter(labels.items()):
        assert b.labels.get(key) == val
    assert b.labels.get(RECURRING_JOB_LABEL) == RECURRING_JOB_NAME
    if base_image:
        assert b.labels.get(BASE_IMAGE_LABEL) == base_image
        # One extra Label from the BaseImage being set.
        assert len(b.labels) == len(labels) + 2
    else:
        # At least one extra Label from RecurringJob.
        assert len(b.labels) == len(labels) + 1

    cleanup_volume(client, volume)


@pytest.mark.csi
@pytest.mark.recurring_job
def test_recurring_job_kubernetes_status(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test RecurringJob properly backs up the KubernetesStatus

    1. Setup a random backupstore.
    2. Create a volume.
    3. Create a PV from the volume, and verify the PV status.
    4. Create a backup recurring job to run every 2 minutes.
    5. Verify the recurring job runs correctly.
    6. Verify the backup contains the Kubernetes Status labels
    &#34;&#34;&#34;
    set_random_backupstore(client)
    host_id = get_self_host_id()
    client.create_volume(name=volume_name, size=SIZE,
                         numberOfReplicas=2)
    volume = common.wait_for_volume_detached(client, volume_name)

    pv_name = &#34;pv-&#34; + volume_name
    common.create_pv_for_volume(client, core_api, volume, pv_name)
    ks = {
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;pvcName&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;lastPodRefAt&#39;: &#39;&#39;,
    }
    wait_volume_kubernetes_status(client, volume_name, ks)

    # Simple Backup Job that runs every 2 minutes, retains 1.
    jobs = [
        {
            &#34;name&#34;: RECURRING_JOB_NAME,
            &#34;cron&#34;: &#34;*/2 * * * *&#34;,
            &#34;task&#34;: &#34;backup&#34;,
            &#34;retain&#34;: 1
        }
    ]
    volume.recurringUpdate(jobs=jobs)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    write_volume_random_data(volume)
    # 5 minutes
    time.sleep(300)
    snapshots = volume.snapshotList()
    count = 0
    for snapshot in snapshots:
        if snapshot.removed is False:
            count += 1
    # 1 from Backup, 1 from Volume Head.
    assert count == 2

    # Verify the Labels on the actual Backup.
    bv = client.by_id_backupVolume(volume_name)
    backups = bv.backupList().data
    assert len(backups) == 1

    b = bv.backupGet(name=backups[0].name)
    status = json.loads(b.labels.get(KUBERNETES_STATUS_LABEL))
    assert b.labels.get(RECURRING_JOB_LABEL) == RECURRING_JOB_NAME
    assert status == {
        &#39;lastPodRefAt&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;pvcName&#39;: &#39;&#39;,
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;workloadsStatus&#39;: None
    }
    # Two Labels: KubernetesStatus and RecurringJob.
    assert len(b.labels) == 2

    cleanup_volume(client, volume)
    delete_and_wait_pv(core_api, pv_name)


def test_recurring_jobs_maximum_retain(client, core_api, volume_name): # NOQA
    &#34;&#34;&#34;
    Test recurring jobs&#39; maximum retain

    1. Create two jobs, with retain 30 and 21.
    2. Try to apply the jobs to a volume. It should fail.
    3. Reduce retain to 30 and 20.
    4. Now the jobs can be applied the volume.
    &#34;&#34;&#34;
    volume = client.create_volume(name=volume_name)

    volume = common.wait_for_volume_detached(client, volume_name)

    jobs = create_jobs1()

    # set max total number of retain to exceed 50
    jobs[0][&#39;retain&#39;] = 30
    jobs[1][&#39;retain&#39;] = 21

    host_id = get_self_host_id()

    volume = volume.attach(hostId=host_id)

    volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        volume.recurringUpdate(jobs=jobs)

    assert &#34;Job Can\\&#39;t retain more than 50 snapshots&#34; in str(e.value)

    jobs[1][&#39;retain&#39;] = 20

    volume = volume.recurringUpdate(jobs=jobs)

    assert len(volume.recurringJobs) == 2
    assert volume.recurringJobs[0][&#39;retain&#39;] == 30
    assert volume.recurringJobs[1][&#39;retain&#39;] == 20


def test_recurring_jobs_for_detached_volume(
    client, core_api, apps_api, volume_name, make_deployment_with_pvc):  # NOQA
    &#34;&#34;&#34;
    Test recurring jobs for detached volume

    Context:
    In the current Longhorn implementation, users cannot do recurring
    backup when volumes are detached.
    This feature gives the users an option to do recurring backup even when
    volumes are detached.
    longhorn/longhorn#1509

    Steps:
    1.  Change the setting allow-recurring-job-while-volume-detached to true.
    2.  Create and attach volume, write 50MB data to the volume.
    3.  Detach the volume.
    4.  Set the recurring backup for the volume on every minute.
    5.  In a 2-minutes retry loop, verify that there is exactly 1 new backup.
    6.  Delete the recurring backup.
    7.  Create a PV and PVC from the volume.
    8.  Create a deployment of 1 pod using the PVC.
    9.  Write 400MB data to the volume from the pod.
    10. Scale down the deployment. Wait until the volume is detached.
    11. Set the recurring backup for every 2 minutes.
    12. Wait util the recurring backup starts, scale up the deployment to 1
        pod.
    13. Verify that during the recurring backup, the volume&#39;s frontend is
        disabled, and pod cannot start.
    14. Wait for the recurring backup finishes.
        Delete the recurring backup.
    15. In a 10-minutes retry loop, verify that the pod can eventually start.
    16. Change the setting allow-recurring-job-while-volume-detached to false.
    17. Cleanup.
    &#34;&#34;&#34;
    set_random_backupstore(client)

    recurring_job_setting = \
        client.by_id_setting(SETTING_RECURRING_JOB_WHILE_VOLUME_DETACHED)
    client.update(recurring_job_setting, value=&#34;true&#34;)

    vol = common.create_and_check_volume(client, volume_name, size=str(1 * Gi))

    lht_hostId = get_self_host_id()
    vol.attach(hostId=lht_hostId)
    vol = common.wait_for_volume_healthy(client, vol.name)

    data = {
        &#39;pos&#39;: 0,
        &#39;content&#39;: common.generate_random_data(50 * Mi),
    }
    common.write_volume_data(vol, data)

    # Give sometimes for data to flush to disk
    time.sleep(15)

    vol.detach()
    vol = common.wait_for_volume_detached(client, vol.name)

    jobs = [
        {
            &#34;name&#34;: RECURRING_JOB_NAME,
            &#34;cron&#34;: &#34;*/1 * * * *&#34;,
            &#34;task&#34;: &#34;backup&#34;,
            &#34;retain&#34;: 1
        }
    ]
    vol.recurringUpdate(jobs=jobs)
    common.wait_for_backup_completion(client, vol.name)
    for _ in range(4):
        bv = client.by_id_backupVolume(vol.name)
        backups = bv.backupList().data
        assert len(backups) == 1
        time.sleep(30)

    vol.recurringUpdate(jobs=[])

    pv_name = volume_name + &#34;-pv&#34;
    common.create_pv_for_volume(client, core_api, vol, pv_name)

    pvc_name = volume_name + &#34;-pvc&#34;
    common.create_pvc_for_volume(client, core_api, vol, pvc_name)

    deployment_name = volume_name + &#34;-dep&#34;
    deployment = make_deployment_with_pvc(deployment_name, pvc_name)
    common.create_and_wait_deployment(apps_api, deployment)

    size_mb = 400
    pod_names = common.get_deployment_pod_names(core_api, deployment)
    write_pod_volume_random_data(core_api, pod_names[0], &#34;/data/test&#34;,
                                 size_mb)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 0
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])

    vol = common.wait_for_volume_detached(client, vol.name)

    jobs = [
        {
            &#34;name&#34;: RECURRING_JOB_NAME,
            &#34;cron&#34;: &#34;*/2 * * * *&#34;,
            &#34;task&#34;: &#34;backup&#34;,
            &#34;retain&#34;: 1
        }
    ]
    vol.recurringUpdate(jobs=jobs)

    common.wait_for_backup_to_start(client, vol.name)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 1
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])

    deployment_label_name = deployment[&#34;metadata&#34;][&#34;labels&#34;][&#34;name&#34;]
    common.wait_pod_auto_attach_after_first_backup_completion(
        client, core_api, vol.name, deployment_label_name)

    vol.recurringUpdate(jobs=[])

    pod_names = common.get_deployment_pod_names(core_api, deployment)
    common.wait_for_pod_phase(core_api, pod_names[0], pod_phase=&#34;Running&#34;)


@pytest.mark.skip(reason=&#34;TODO&#34;)
def test_recurring_jobs_on_nodes_with_taints():  # NOQA
    &#34;&#34;&#34;
    Test recurring jobs on nodes with taints

    Context:

    Test the prevention of creation of multiple pods due to
    recurring job&#39;s pod being rejected by Taint controller
    on nodes with taints

    Steps:

    1. Set taint toleration for Longhorn components
       `persistence=true:NoExecute`
    2. Taint `node-1` with `persistence=true:NoExecute`
    3. Create a volume, vol-1.
       Attach vol-1 to node-1
       Write some data to vol-1
    4. Create a recurring backup job which:
       Has retain count 10
       Runs every minute
    5. Wait for 3 minutes.
       Verify that the there is 1 backup created
       Verify that the total number of pod in longhorn-system namespace &lt; 50
       Verify that the number of pods of the cronjob is &lt;= 2

    6. Taint all nodes with `persistence=true:NoExecute`
    7. Write some data to vol-1
    8. Wait for 3 minutes.
       Verify that the there are 2 backups created in total
       Verify that the total number of pod in longhorn-system namespace &lt; 50
       Verify that the number of pods of the cronjob is &lt;= 2

    9. Remove `persistence=true:NoExecute` from all nodes and Longhorn setting
       Clean up backups, volumes
    &#34;&#34;&#34;
    pass


@pytest.mark.skip(reason=&#34;TODO&#34;)
def test_recurring_jobs_when_volume_detached_unexpectedly():  # NOQA
    &#34;&#34;&#34;
    Test recurring jobs when volume detached unexpectedly

    Context:

    If the volume is automatically attached by the recurring backup job,
    make sure that workload pod eventually is able to use the volume
    when volume is detached unexpectedly during the backup process.

    Steps:

    1. Create a volume, attach to a pod of a deployment,
       write 300MB to the volume
    2. Scale down the deployment. The volume is detached.
    3. Turn on `Allow Recurring Job While Volume Is Detached` setting
    4. Create a recurring backup job that runs every 4 mins
    5. Wait until the recurring backup job starts and the backup progress
       is &gt; 50%, kill the engine process of the volume.
    6. In a 2-min retry loop, verify that the volume is healthy again
       and there is replacement backup job running.
    7. Wait until the backup finishes.
    8. Wait for the volume to be in detached state with
       `frontendDisabled=false`
    9. Scale up the deployment.
       Verify that we can read the file `lost+found` from the workload pod
    10. Turn off `Allow Recurring Job While Volume Is Detached` setting
       Clean up backups, volumes
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_recurring_job.check_jobs1_result"><code class="name flex">
<span>def <span class="ident">check_jobs1_result</span></span>(<span>volume)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_jobs1_result(volume):
    snapshots = volume.snapshotList()
    count = 0
    for snapshot in snapshots:
        if snapshot.removed is False:
            count += 1
    # 2 snapshots, 1 backup, 1 volume-head
    assert count == 4</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.create_jobs1"><code class="name flex">
<span>def <span class="ident">create_jobs1</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_jobs1():
    # snapshot every one minute
    job_snap = {&#34;name&#34;: &#34;snap&#34;, &#34;cron&#34;: &#34;* * * * *&#34;,
                &#34;task&#34;: &#34;snapshot&#34;, &#34;retain&#34;: 2}
    # backup every two minutes
    job_backup = {&#34;name&#34;: &#34;backup&#34;, &#34;cron&#34;: &#34;*/2 * * * *&#34;,
                  &#34;task&#34;: &#34;backup&#34;, &#34;retain&#34;: 1}
    return [job_snap, job_backup]</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.recurring_job_labels_test"><code class="name flex">
<span>def <span class="ident">recurring_job_labels_test</span></span>(<span>client, labels, volume_name, size='16777216', base_image='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def recurring_job_labels_test(client, labels, volume_name, size=SIZE, base_image=&#34;&#34;):  # NOQA
    set_random_backupstore(client)
    host_id = get_self_host_id()
    client.create_volume(name=volume_name, size=size,
                         numberOfReplicas=2)
    volume = common.wait_for_volume_detached(client, volume_name)

    # Simple Backup Job that runs every 1 minute, retains 1.
    jobs = [
        {
            &#34;name&#34;: RECURRING_JOB_NAME,
            &#34;cron&#34;: &#34;*/1 * * * *&#34;,
            &#34;task&#34;: &#34;backup&#34;,
            &#34;retain&#34;: 1,
            &#34;labels&#34;: labels
        }
    ]
    volume.recurringUpdate(jobs=jobs)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)
    write_volume_random_data(volume)

    # 1 minutes 15s
    time.sleep(75)
    labels[&#34;we-added-this-label&#34;] = &#34;definitely&#34;
    jobs[0][&#34;labels&#34;] = labels
    volume = volume.recurringUpdate(jobs=jobs)
    volume = common.wait_for_volume_healthy(client, volume_name)
    write_volume_random_data(volume)

    # 2 minutes 15s
    time.sleep(135)
    snapshots = volume.snapshotList()
    count = 0
    for snapshot in snapshots:
        if snapshot.removed is False:
            count += 1
    # 1 from Backup, 1 from Volume Head.
    assert count == 2

    # Verify the Labels on the actual Backup.
    bv = client.by_id_backupVolume(volume_name)
    backups = bv.backupList().data
    assert len(backups) == 1

    b = bv.backupGet(name=backups[0].name)
    for key, val in iter(labels.items()):
        assert b.labels.get(key) == val
    assert b.labels.get(RECURRING_JOB_LABEL) == RECURRING_JOB_NAME
    if base_image:
        assert b.labels.get(BASE_IMAGE_LABEL) == base_image
        # One extra Label from the BaseImage being set.
        assert len(b.labels) == len(labels) + 2
    else:
        # At least one extra Label from RecurringJob.
        assert len(b.labels) == len(labels) + 1

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job"><code class="name flex">
<span>def <span class="ident">test_recurring_job</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test recurring job</p>
<ol>
<li>Setup a random backupstore</li>
<li>Create a volume.</li>
<li>Create two jobs
1 job 1: snapshot every one minute, retain 2
1 job 2: backup every two minutes, retain 1</li>
<li>Attach the volume.
Wait until the 10th second since the beginning of an even minute</li>
<li>Write some data. Sleep 2.5 minutes.
Write some data. Sleep 2.5 minutes</li>
<li>Verify we have 4 snapshots total<ol>
<li>2 snapshots, 1 backup, 1 volume-head</li>
</ol>
</li>
<li>Update jobs to replace the backup job<ol>
<li>New backup job run every one minute, retain 2</li>
</ol>
</li>
<li>Write some data. Sleep 2.5 minutes.
Write some data. Sleep 2.5 minutes</li>
<li>We should have 6 snapshots<ol>
<li>2 from job_snap, 1 from job_backup, 2 from job_backup2, 1
volume-head</li>
</ol>
</li>
<li>Make sure there are exactly 4 completed backups.<ol>
<li>old backup job completed 2 backups</li>
<li>new backup job completed 2 backups</li>
</ol>
</li>
<li>Make sure we have no backup in progress</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test recurring job

    1. Setup a random backupstore
    2. Create a volume.
    3. Create two jobs
        1 job 1: snapshot every one minute, retain 2
        1 job 2: backup every two minutes, retain 1
    4. Attach the volume.
       Wait until the 10th second since the beginning of an even minute
    5. Write some data. Sleep 2.5 minutes.
       Write some data. Sleep 2.5 minutes
    6. Verify we have 4 snapshots total
        1. 2 snapshots, 1 backup, 1 volume-head
    7. Update jobs to replace the backup job
        1. New backup job run every one minute, retain 2
    8. Write some data. Sleep 2.5 minutes.
       Write some data. Sleep 2.5 minutes
    9. We should have 6 snapshots
        1. 2 from job_snap, 1 from job_backup, 2 from job_backup2, 1
        volume-head
    10. Make sure there are exactly 4 completed backups.
        1. old backup job completed 2 backups
        2. new backup job completed 2 backups
    11. Make sure we have no backup in progress
    &#34;&#34;&#34;

    &#39;&#39;&#39;
    The timeline looks like this:
    0   1   2   3   4   5   6   7   8   9   10     (minute)
    |W  |   | W |   |   |W  |   | W |   |   |      (write data)
    |   S   |   S   |   |   S   |   S   |   |      (job_snap)
    |   |   B   |   B   |   |   |   |   |   |      (job_backup1)
    |   |   |   |   |   |   B   |   B   |   |      (job_backup2)
    &#39;&#39;&#39;

    host_id = get_self_host_id()

    set_random_backupstore(client)

    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=2)
    volume = common.wait_for_volume_detached(client, volume_name)

    jobs = create_jobs1()
    volume.recurringUpdate(jobs=jobs)

    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    # wait until the beginning of an even minute
    wait_until_begin_of_an_even_minute()
    # wait until the 10th second of an even minute
    # make sure that snapshot job happens before the backup job
    time.sleep(10)

    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes
    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes

    check_jobs1_result(volume)

    job_backup2 = {&#34;name&#34;: &#34;backup2&#34;, &#34;cron&#34;: &#34;* * * * *&#34;,
                   &#34;task&#34;: &#34;backup&#34;, &#34;retain&#34;: 2}
    volume.recurringUpdate(jobs=[jobs[0], job_backup2])

    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes
    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes

    snapshots = volume.snapshotList()
    count = 0
    for snapshot in snapshots:
        if snapshot.removed is False:
            count += 1
    # 2 from job_snap, 1 from job_backup, 2 from job_backup2, 1 volume-head
    assert count == 6

    complete_backup_number = 0
    in_progress_backup_number = 0
    volume = client.by_id_volume(volume_name)
    for b in volume.backupStatus:
        assert b.error == &#34;&#34;
        if b.state == &#34;complete&#34;:
            complete_backup_number += 1
        elif b.state == &#34;in_progress&#34;:
            in_progress_backup_number += 1

    # 2 completed backups from job_backup
    # 2 completed backups from job_backup2
    assert complete_backup_number == 4

    assert in_progress_backup_number == 0

    volume = volume.detach()

    common.wait_for_volume_detached(client, volume_name)

    client.delete(volume)

    wait_for_volume_delete(client, volume_name)

    volumes = client.list_volume()
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_in_storageclass"><code class="name flex">
<span>def <span class="ident">test_recurring_job_in_storageclass</span></span>(<span>client, core_api, storage_class, statefulset)</span>
</code></dt>
<dd>
<div class="desc"><p>Test create volume with StorageClass contains recurring jobs</p>
<ol>
<li>Create a StorageClass with recurring jobs</li>
<li>Create a StatefulSet with PVC template and StorageClass</li>
<li>Verify the recurring jobs run correctly.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_in_storageclass(client, core_api, storage_class, statefulset):  # NOQA
    &#34;&#34;&#34;
    Test create volume with StorageClass contains recurring jobs

    1. Create a StorageClass with recurring jobs
    2. Create a StatefulSet with PVC template and StorageClass
    3. Verify the recurring jobs run correctly.
    &#34;&#34;&#34;
    set_random_backupstore(client)
    statefulset_name = &#39;recurring-job-in-storageclass-test&#39;
    update_statefulset_manifests(statefulset, storage_class, statefulset_name)
    storage_class[&#34;parameters&#34;][&#34;recurringJobs&#34;] = json.dumps(create_jobs1())

    create_storage_class(storage_class)

    # wait until the beginning of an even minute
    wait_until_begin_of_an_even_minute()

    start_time = datetime.utcnow()
    create_and_wait_statefulset(statefulset)
    statefulset_creating_duration = datetime.utcnow() - start_time

    assert 150 &gt; statefulset_creating_duration.seconds

    # We want to write data exactly at the 150th second since the start_time
    time.sleep(150 - statefulset_creating_duration.seconds)

    pod_info = get_statefulset_pod_info(core_api, statefulset)
    volume_info = [p[&#39;pv_name&#39;] for p in pod_info]
    pod_names = [p[&#39;pod_name&#39;] for p in pod_info]

    # write random data to volume to trigger recurring snapshot and backup job
    volume_data_path = &#34;/data/test&#34;
    for pod_name in pod_names:
        write_pod_volume_random_data(core_api, pod_name, volume_data_path, 2)

    time.sleep(150)  # 2.5 minutes

    for volume_name in volume_info:  # NOQA
        volume = client.by_id_volume(volume_name)
        check_jobs1_result(volume)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_in_volume_creation"><code class="name flex">
<span>def <span class="ident">test_recurring_job_in_volume_creation</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test create volume with recurring jobs</p>
<ol>
<li>Create volume with recurring jobs though Longhorn API</li>
<li>Verify the recurring jobs run correctly</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_in_volume_creation(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test create volume with recurring jobs

    1. Create volume with recurring jobs though Longhorn API
    2. Verify the recurring jobs run correctly
    &#34;&#34;&#34;
    host_id = get_self_host_id()

    set_random_backupstore(client)

    # error when creating volume with duplicate jobs
    with pytest.raises(Exception) as e:
        client.create_volume(name=volume_name, size=SIZE,
                             numberOfReplicas=2,
                             recurringJobs=create_jobs1() + create_jobs1())
    assert &#34;duplicate job&#34; in str(e.value)

    client.create_volume(name=volume_name, size=SIZE,
                         numberOfReplicas=2, recurringJobs=create_jobs1())
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    # wait until the beginning of an even minute
    wait_until_begin_of_an_even_minute()
    # wait until the 10th second of an even minute
    # to avoid writing data at the same time backup is taking
    time.sleep(10)

    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes
    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes

    check_jobs1_result(volume)

    volume = volume.detach()
    common.wait_for_volume_detached(client, volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    volumes = client.list_volume()
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_kubernetes_status"><code class="name flex">
<span>def <span class="ident">test_recurring_job_kubernetes_status</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test RecurringJob properly backs up the KubernetesStatus</p>
<ol>
<li>Setup a random backupstore.</li>
<li>Create a volume.</li>
<li>Create a PV from the volume, and verify the PV status.</li>
<li>Create a backup recurring job to run every 2 minutes.</li>
<li>Verify the recurring job runs correctly.</li>
<li>Verify the backup contains the Kubernetes Status labels</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.csi
@pytest.mark.recurring_job
def test_recurring_job_kubernetes_status(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test RecurringJob properly backs up the KubernetesStatus

    1. Setup a random backupstore.
    2. Create a volume.
    3. Create a PV from the volume, and verify the PV status.
    4. Create a backup recurring job to run every 2 minutes.
    5. Verify the recurring job runs correctly.
    6. Verify the backup contains the Kubernetes Status labels
    &#34;&#34;&#34;
    set_random_backupstore(client)
    host_id = get_self_host_id()
    client.create_volume(name=volume_name, size=SIZE,
                         numberOfReplicas=2)
    volume = common.wait_for_volume_detached(client, volume_name)

    pv_name = &#34;pv-&#34; + volume_name
    common.create_pv_for_volume(client, core_api, volume, pv_name)
    ks = {
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;pvcName&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;lastPodRefAt&#39;: &#39;&#39;,
    }
    wait_volume_kubernetes_status(client, volume_name, ks)

    # Simple Backup Job that runs every 2 minutes, retains 1.
    jobs = [
        {
            &#34;name&#34;: RECURRING_JOB_NAME,
            &#34;cron&#34;: &#34;*/2 * * * *&#34;,
            &#34;task&#34;: &#34;backup&#34;,
            &#34;retain&#34;: 1
        }
    ]
    volume.recurringUpdate(jobs=jobs)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    write_volume_random_data(volume)
    # 5 minutes
    time.sleep(300)
    snapshots = volume.snapshotList()
    count = 0
    for snapshot in snapshots:
        if snapshot.removed is False:
            count += 1
    # 1 from Backup, 1 from Volume Head.
    assert count == 2

    # Verify the Labels on the actual Backup.
    bv = client.by_id_backupVolume(volume_name)
    backups = bv.backupList().data
    assert len(backups) == 1

    b = bv.backupGet(name=backups[0].name)
    status = json.loads(b.labels.get(KUBERNETES_STATUS_LABEL))
    assert b.labels.get(RECURRING_JOB_LABEL) == RECURRING_JOB_NAME
    assert status == {
        &#39;lastPodRefAt&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;pvcName&#39;: &#39;&#39;,
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;workloadsStatus&#39;: None
    }
    # Two Labels: KubernetesStatus and RecurringJob.
    assert len(b.labels) == 2

    cleanup_volume(client, volume)
    delete_and_wait_pv(core_api, pv_name)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_labels"><code class="name flex">
<span>def <span class="ident">test_recurring_job_labels</span></span>(<span>client, random_labels, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test a RecurringJob with labels</p>
<ol>
<li>Set a random backupstore</li>
<li>Create a backup recurring job with labels</li>
<li>Wait for job to create a backup</li>
<li>Add a label to the job</li>
<li>Verify the recurring jobs run correctly.</li>
<li>Verify the labels on the backup are correct.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job
def test_recurring_job_labels(client, random_labels, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test a RecurringJob with labels

    1. Set a random backupstore
    2. Create a backup recurring job with labels
    3. Wait for job to create a backup
    4. Add a label to the job
    5. Verify the recurring jobs run correctly.
    6. Verify the labels on the backup are correct.
    &#34;&#34;&#34;
    recurring_job_labels_test(client, random_labels, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_jobs_for_detached_volume"><code class="name flex">
<span>def <span class="ident">test_recurring_jobs_for_detached_volume</span></span>(<span>client, core_api, apps_api, volume_name, make_deployment_with_pvc)</span>
</code></dt>
<dd>
<div class="desc"><p>Test recurring jobs for detached volume</p>
<p>Context:
In the current Longhorn implementation, users cannot do recurring
backup when volumes are detached.
This feature gives the users an option to do recurring backup even when
volumes are detached.
longhorn/longhorn#1509</p>
<p>Steps:
1.
Change the setting allow-recurring-job-while-volume-detached to true.
2.
Create and attach volume, write 50MB data to the volume.
3.
Detach the volume.
4.
Set the recurring backup for the volume on every minute.
5.
In a 2-minutes retry loop, verify that there is exactly 1 new backup.
6.
Delete the recurring backup.
7.
Create a PV and PVC from the volume.
8.
Create a deployment of 1 pod using the PVC.
9.
Write 400MB data to the volume from the pod.
10. Scale down the deployment. Wait until the volume is detached.
11. Set the recurring backup for every 2 minutes.
12. Wait util the recurring backup starts, scale up the deployment to 1
pod.
13. Verify that during the recurring backup, the volume's frontend is
disabled, and pod cannot start.
14. Wait for the recurring backup finishes.
Delete the recurring backup.
15. In a 10-minutes retry loop, verify that the pod can eventually start.
16. Change the setting allow-recurring-job-while-volume-detached to false.
17. Cleanup.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_recurring_jobs_for_detached_volume(
    client, core_api, apps_api, volume_name, make_deployment_with_pvc):  # NOQA
    &#34;&#34;&#34;
    Test recurring jobs for detached volume

    Context:
    In the current Longhorn implementation, users cannot do recurring
    backup when volumes are detached.
    This feature gives the users an option to do recurring backup even when
    volumes are detached.
    longhorn/longhorn#1509

    Steps:
    1.  Change the setting allow-recurring-job-while-volume-detached to true.
    2.  Create and attach volume, write 50MB data to the volume.
    3.  Detach the volume.
    4.  Set the recurring backup for the volume on every minute.
    5.  In a 2-minutes retry loop, verify that there is exactly 1 new backup.
    6.  Delete the recurring backup.
    7.  Create a PV and PVC from the volume.
    8.  Create a deployment of 1 pod using the PVC.
    9.  Write 400MB data to the volume from the pod.
    10. Scale down the deployment. Wait until the volume is detached.
    11. Set the recurring backup for every 2 minutes.
    12. Wait util the recurring backup starts, scale up the deployment to 1
        pod.
    13. Verify that during the recurring backup, the volume&#39;s frontend is
        disabled, and pod cannot start.
    14. Wait for the recurring backup finishes.
        Delete the recurring backup.
    15. In a 10-minutes retry loop, verify that the pod can eventually start.
    16. Change the setting allow-recurring-job-while-volume-detached to false.
    17. Cleanup.
    &#34;&#34;&#34;
    set_random_backupstore(client)

    recurring_job_setting = \
        client.by_id_setting(SETTING_RECURRING_JOB_WHILE_VOLUME_DETACHED)
    client.update(recurring_job_setting, value=&#34;true&#34;)

    vol = common.create_and_check_volume(client, volume_name, size=str(1 * Gi))

    lht_hostId = get_self_host_id()
    vol.attach(hostId=lht_hostId)
    vol = common.wait_for_volume_healthy(client, vol.name)

    data = {
        &#39;pos&#39;: 0,
        &#39;content&#39;: common.generate_random_data(50 * Mi),
    }
    common.write_volume_data(vol, data)

    # Give sometimes for data to flush to disk
    time.sleep(15)

    vol.detach()
    vol = common.wait_for_volume_detached(client, vol.name)

    jobs = [
        {
            &#34;name&#34;: RECURRING_JOB_NAME,
            &#34;cron&#34;: &#34;*/1 * * * *&#34;,
            &#34;task&#34;: &#34;backup&#34;,
            &#34;retain&#34;: 1
        }
    ]
    vol.recurringUpdate(jobs=jobs)
    common.wait_for_backup_completion(client, vol.name)
    for _ in range(4):
        bv = client.by_id_backupVolume(vol.name)
        backups = bv.backupList().data
        assert len(backups) == 1
        time.sleep(30)

    vol.recurringUpdate(jobs=[])

    pv_name = volume_name + &#34;-pv&#34;
    common.create_pv_for_volume(client, core_api, vol, pv_name)

    pvc_name = volume_name + &#34;-pvc&#34;
    common.create_pvc_for_volume(client, core_api, vol, pvc_name)

    deployment_name = volume_name + &#34;-dep&#34;
    deployment = make_deployment_with_pvc(deployment_name, pvc_name)
    common.create_and_wait_deployment(apps_api, deployment)

    size_mb = 400
    pod_names = common.get_deployment_pod_names(core_api, deployment)
    write_pod_volume_random_data(core_api, pod_names[0], &#34;/data/test&#34;,
                                 size_mb)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 0
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])

    vol = common.wait_for_volume_detached(client, vol.name)

    jobs = [
        {
            &#34;name&#34;: RECURRING_JOB_NAME,
            &#34;cron&#34;: &#34;*/2 * * * *&#34;,
            &#34;task&#34;: &#34;backup&#34;,
            &#34;retain&#34;: 1
        }
    ]
    vol.recurringUpdate(jobs=jobs)

    common.wait_for_backup_to_start(client, vol.name)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 1
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])

    deployment_label_name = deployment[&#34;metadata&#34;][&#34;labels&#34;][&#34;name&#34;]
    common.wait_pod_auto_attach_after_first_backup_completion(
        client, core_api, vol.name, deployment_label_name)

    vol.recurringUpdate(jobs=[])

    pod_names = common.get_deployment_pod_names(core_api, deployment)
    common.wait_for_pod_phase(core_api, pod_names[0], pod_phase=&#34;Running&#34;)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_jobs_maximum_retain"><code class="name flex">
<span>def <span class="ident">test_recurring_jobs_maximum_retain</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test recurring jobs' maximum retain</p>
<ol>
<li>Create two jobs, with retain 30 and 21.</li>
<li>Try to apply the jobs to a volume. It should fail.</li>
<li>Reduce retain to 30 and 20.</li>
<li>Now the jobs can be applied the volume.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_recurring_jobs_maximum_retain(client, core_api, volume_name): # NOQA
    &#34;&#34;&#34;
    Test recurring jobs&#39; maximum retain

    1. Create two jobs, with retain 30 and 21.
    2. Try to apply the jobs to a volume. It should fail.
    3. Reduce retain to 30 and 20.
    4. Now the jobs can be applied the volume.
    &#34;&#34;&#34;
    volume = client.create_volume(name=volume_name)

    volume = common.wait_for_volume_detached(client, volume_name)

    jobs = create_jobs1()

    # set max total number of retain to exceed 50
    jobs[0][&#39;retain&#39;] = 30
    jobs[1][&#39;retain&#39;] = 21

    host_id = get_self_host_id()

    volume = volume.attach(hostId=host_id)

    volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        volume.recurringUpdate(jobs=jobs)

    assert &#34;Job Can\\&#39;t retain more than 50 snapshots&#34; in str(e.value)

    jobs[1][&#39;retain&#39;] = 20

    volume = volume.recurringUpdate(jobs=jobs)

    assert len(volume.recurringJobs) == 2
    assert volume.recurringJobs[0][&#39;retain&#39;] == 30
    assert volume.recurringJobs[1][&#39;retain&#39;] == 20</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_jobs_on_nodes_with_taints"><code class="name flex">
<span>def <span class="ident">test_recurring_jobs_on_nodes_with_taints</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Test recurring jobs on nodes with taints</p>
<p>Context:</p>
<p>Test the prevention of creation of multiple pods due to
recurring job's pod being rejected by Taint controller
on nodes with taints</p>
<p>Steps:</p>
<ol>
<li>Set taint toleration for Longhorn components
<code>persistence=true:NoExecute</code></li>
<li>Taint <code>node-1</code> with <code>persistence=true:NoExecute</code></li>
<li>Create a volume, vol-1.
Attach vol-1 to node-1
Write some data to vol-1</li>
<li>Create a recurring backup job which:
Has retain count 10
Runs every minute</li>
<li>
<p>Wait for 3 minutes.
Verify that the there is 1 backup created
Verify that the total number of pod in longhorn-system namespace &lt; 50
Verify that the number of pods of the cronjob is &lt;= 2</p>
</li>
<li>
<p>Taint all nodes with <code>persistence=true:NoExecute</code></p>
</li>
<li>Write some data to vol-1</li>
<li>
<p>Wait for 3 minutes.
Verify that the there are 2 backups created in total
Verify that the total number of pod in longhorn-system namespace &lt; 50
Verify that the number of pods of the cronjob is &lt;= 2</p>
</li>
<li>
<p>Remove <code>persistence=true:NoExecute</code> from all nodes and Longhorn setting
Clean up backups, volumes</p>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(reason=&#34;TODO&#34;)
def test_recurring_jobs_on_nodes_with_taints():  # NOQA
    &#34;&#34;&#34;
    Test recurring jobs on nodes with taints

    Context:

    Test the prevention of creation of multiple pods due to
    recurring job&#39;s pod being rejected by Taint controller
    on nodes with taints

    Steps:

    1. Set taint toleration for Longhorn components
       `persistence=true:NoExecute`
    2. Taint `node-1` with `persistence=true:NoExecute`
    3. Create a volume, vol-1.
       Attach vol-1 to node-1
       Write some data to vol-1
    4. Create a recurring backup job which:
       Has retain count 10
       Runs every minute
    5. Wait for 3 minutes.
       Verify that the there is 1 backup created
       Verify that the total number of pod in longhorn-system namespace &lt; 50
       Verify that the number of pods of the cronjob is &lt;= 2

    6. Taint all nodes with `persistence=true:NoExecute`
    7. Write some data to vol-1
    8. Wait for 3 minutes.
       Verify that the there are 2 backups created in total
       Verify that the total number of pod in longhorn-system namespace &lt; 50
       Verify that the number of pods of the cronjob is &lt;= 2

    9. Remove `persistence=true:NoExecute` from all nodes and Longhorn setting
       Clean up backups, volumes
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_jobs_when_volume_detached_unexpectedly"><code class="name flex">
<span>def <span class="ident">test_recurring_jobs_when_volume_detached_unexpectedly</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Test recurring jobs when volume detached unexpectedly</p>
<p>Context:</p>
<p>If the volume is automatically attached by the recurring backup job,
make sure that workload pod eventually is able to use the volume
when volume is detached unexpectedly during the backup process.</p>
<p>Steps:</p>
<ol>
<li>Create a volume, attach to a pod of a deployment,
write 300MB to the volume</li>
<li>Scale down the deployment. The volume is detached.</li>
<li>Turn on <code>Allow Recurring Job While Volume Is Detached</code> setting</li>
<li>Create a recurring backup job that runs every 4 mins</li>
<li>Wait until the recurring backup job starts and the backup progress
is &gt; 50%, kill the engine process of the volume.</li>
<li>In a 2-min retry loop, verify that the volume is healthy again
and there is replacement backup job running.</li>
<li>Wait until the backup finishes.</li>
<li>Wait for the volume to be in detached state with
<code>frontendDisabled=false</code></li>
<li>Scale up the deployment.
Verify that we can read the file <code>lost+found</code> from the workload pod</li>
<li>Turn off <code>Allow Recurring Job While Volume Is Detached</code> setting
Clean up backups, volumes</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(reason=&#34;TODO&#34;)
def test_recurring_jobs_when_volume_detached_unexpectedly():  # NOQA
    &#34;&#34;&#34;
    Test recurring jobs when volume detached unexpectedly

    Context:

    If the volume is automatically attached by the recurring backup job,
    make sure that workload pod eventually is able to use the volume
    when volume is detached unexpectedly during the backup process.

    Steps:

    1. Create a volume, attach to a pod of a deployment,
       write 300MB to the volume
    2. Scale down the deployment. The volume is detached.
    3. Turn on `Allow Recurring Job While Volume Is Detached` setting
    4. Create a recurring backup job that runs every 4 mins
    5. Wait until the recurring backup job starts and the backup progress
       is &gt; 50%, kill the engine process of the volume.
    6. In a 2-min retry loop, verify that the volume is healthy again
       and there is replacement backup job running.
    7. Wait until the backup finishes.
    8. Wait for the volume to be in detached state with
       `frontendDisabled=false`
    9. Scale up the deployment.
       Verify that we can read the file `lost+found` from the workload pod
    10. Turn off `Allow Recurring Job While Volume Is Detached` setting
       Clean up backups, volumes
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.wait_until_begin_of_an_even_minute"><code class="name flex">
<span>def <span class="ident">wait_until_begin_of_an_even_minute</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_until_begin_of_an_even_minute():
    while True:
        current_time = datetime.utcnow()
        if current_time.second == 0 and current_time.minute % 2 == 0:
            break
        time.sleep(1)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_recurring_job.check_jobs1_result" href="#tests.test_recurring_job.check_jobs1_result">check_jobs1_result</a></code></li>
<li><code><a title="tests.test_recurring_job.create_jobs1" href="#tests.test_recurring_job.create_jobs1">create_jobs1</a></code></li>
<li><code><a title="tests.test_recurring_job.recurring_job_labels_test" href="#tests.test_recurring_job.recurring_job_labels_test">recurring_job_labels_test</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job" href="#tests.test_recurring_job.test_recurring_job">test_recurring_job</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_in_storageclass" href="#tests.test_recurring_job.test_recurring_job_in_storageclass">test_recurring_job_in_storageclass</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_in_volume_creation" href="#tests.test_recurring_job.test_recurring_job_in_volume_creation">test_recurring_job_in_volume_creation</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_kubernetes_status" href="#tests.test_recurring_job.test_recurring_job_kubernetes_status">test_recurring_job_kubernetes_status</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_labels" href="#tests.test_recurring_job.test_recurring_job_labels">test_recurring_job_labels</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_jobs_for_detached_volume" href="#tests.test_recurring_job.test_recurring_jobs_for_detached_volume">test_recurring_jobs_for_detached_volume</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_jobs_maximum_retain" href="#tests.test_recurring_job.test_recurring_jobs_maximum_retain">test_recurring_jobs_maximum_retain</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_jobs_on_nodes_with_taints" href="#tests.test_recurring_job.test_recurring_jobs_on_nodes_with_taints">test_recurring_jobs_on_nodes_with_taints</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_jobs_when_volume_detached_unexpectedly" href="#tests.test_recurring_job.test_recurring_jobs_when_volume_detached_unexpectedly">test_recurring_jobs_when_volume_detached_unexpectedly</a></code></li>
<li><code><a title="tests.test_recurring_job.wait_until_begin_of_an_even_minute" href="#tests.test_recurring_job.wait_until_begin_of_an_even_minute">wait_until_begin_of_an_even_minute</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>